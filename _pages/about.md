---
layout: archive
permalink: /
title: "Hi, I'm Haokun Lin (æ—æµ©å¤) ğŸ»"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I'm a Ph.D. candidate at [New Laboratory of Pattern Recognition (NLPR)](http://cripac.ia.ac.cn/en/EN/volumn/home.shtml), [Institute of Automation, Chinese Academy of Sciences](https://english.ia.cas.cn/) under the supervision of [Prof. Zhenan Sun](http://www.cbsr.ia.ac.cn/users/znsun/). I'm also a joint Ph.D. candidate at [Department of Computer Science](https://www.cs.cityu.edu.hk/), [City University of Hong Kong](https://www.cityu.edu.hk/), working with [Prof. Ying Wei](https://wei-ying.net/). 
Before joining CASIA, I received both my B.Eng. in Software Engineering from [Huazhong University of Science and Technology](https://english.hust.edu.cn/) in 2021.
 

My research interests include **<u>Multi-modal Learning</u>**, **<u>Large Language/Vision Models</u>**, and **<u>Efficient Deep Learning</u>**.

ğŸ‘‹ğŸ‘‹ğŸ‘‹ If you're interested in my work, please feel free to reach out for discussions or collaborations!

**Contact me via**:  
ğŸ“§ Mail: [haokun.lin[AT]cripac.ia.ac.cn](haokun.lin@cripac.ia.ac.cn) or [haokunlin2-c[AT]my.cityu.edu.hk](haokunlin2-c@my.cityu.edu.hk)

<h1 style="color: rgb(231, 65, 65);">ğŸŒˆ What's new:</h1>

<div style="height: 350px; overflow: auto; border: 1px solid #ccc; margin: 15px;">

<ul>
  <li><strong style="font-family: Consolas;">[11/2024]</strong>  ğŸš€ <b style="color: rgb(231, 165, 65);">Award:</b> Delighted to have received the First Prize in the 2024 Graduate Academic Forum at UCAS!</li>
  <li><strong style="font-family: Consolas;">[11/2024]</strong>  ğŸ“œ <b style="color: rgb(231, 165, 65);">Preprint:</b> "DOGE: Towards Versatile Visual Document Grounding and Referring." [<a href="https://arxiv.org/pdf/2411.17125">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[11/2024]</strong>  ğŸš€ <b style="color: rgb(231, 165, 65);">Award:</b> Honored to be selected as a Top Reviewer at NeurIPS 2024!</li>
 <li><strong style="font-family: Consolas;">[09/2024]</strong>  ğŸ‰ <b style="color: rgb(231, 165, 65);">NeurIPS'24 Oral:</b> "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs." Big Congs! ğŸ”¥ğŸ”¥ğŸ”¥ [<a href="https://github.com/Hsu1023/DuQuant">Code</a>/<a href="https://arxiv.org/pdf/2406.01721">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[07/2024]</strong>  ğŸ‰ <b style="color: rgb(231, 165, 65);">ECCV'24:</b> "MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?" [<a href="https://github.com/ZrrSkywalker/MathVerse">Code</a>/<a href="https://arxiv.org/pdf/2403.14624">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[05/2024]</strong>  ğŸ‰ <b style="color: rgb(231, 165, 65);">ACL'24 Findings:</b> "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact." [<a href="https://github.com/ruikangliu/IntactKV">Code</a>/<a href="https://arxiv.org/pdf/2403.01241">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[02/2024]</strong>  ğŸ‰  <b style="color: rgb(231, 165, 65);">CVPR'24:</b> "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric." [<a href="https://arxiv.org/pdf/2403.07839">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[01/2024]</strong>  ğŸ‰  <b style="color: rgb(231, 165, 65);">ICLR'24:</b> "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models." [<a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Code</a>/<a href="https://openreview.net/pdf?id=Tr0lPx9woF">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[03/2022]</strong>  ğŸ“ <b style="color: rgb(231, 165, 65);">Starting Joint Ph.D.@CityU:</b> I will join Prof. <a href="https://wei-ying.net/">Ying Wei</a>'s group at CityU in 2022 Fall!</li>
  <li><strong style="font-family: Consolas;">[03/2022]</strong>  ğŸ“ <b style="color: rgb(231, 165, 65);">Starting Ph.D.@CASIA:</b> I will join Prof. <a href="http://www.cbsr.ia.ac.cn/users/znsun/">Zhenan Sun</a>'s group at NLPR, CASIA in 2021 Fall!</li>
  <li><strong style="font-family: Consolas;">[06/2021]</strong>  ğŸ“ <b style="color: rgb(231, 165, 65);">Graduation@HUST:</b> Recieved my Bachelor's Degree from Huazhong University of Science and Technology with Honorary degree.</li>
</ul>
</div>



# ğŸ“ Selected Publications ([Google Scholar](https://scholar.google.com/citations?user=7DnpUlIAAAAJ))
($\*$: co-first author;  ^: corresponding author)
<table style="width:100%;border:None;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:0;margin-top:-1.5em;font-size:0.95em;">
  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs.</b> 
      <br>
      <u>Haokun Lin*</u>, Haobo Xu*, Yichen Wu*, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun^, Ying Wei^,
      <br>
      <i>in 38th Conference on Neural Information Processing Systems (<b>NeurIPS 2024 Oral</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2406.01721">arXiv</a>]
      [<a href="https://duquant.github.io/">Project</a>]
      [<a href="https://github.com/Hsu1023/DuQuant">Github</a>]
      [<a href="https://mp.weixin.qq.com/s/lM4HeylIivW8c2o5f6J8wg">QbitAI/é‡å­ä½</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8iXSD-jJ40cJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVBmU:AFWwaeYAAAAAZ1gTHmVTCuwIqDAUyZbuC7QEvnc&scisig=AFWwaeYAAAAAZ1gTHiEDO9ff6IZDdW8xXIXGci8&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric.</b> 
      <br>
      <u>Haokun Lin</u>, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei^, Zhenan Sun^,
      <br>
      <i>in IEEE / CVF Computer Vision and Pattern Recognition Conference 2024 (<b>CVPR 2024</b>).</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.07839">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.07839">arXiv</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8JMVX1X1EywJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVcyM:AFWwaeYAAAAAZ1gTayNSVuBvHTat9NYvvUWKl1I&scisig=AFWwaeYAAAAAZ1gTa2N0QuCB1EVIvzpPzLM_jJ4&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border:none;">
      <b>MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</b> 
      <br>
      Renrui Zhang*, Dongzhi Jiang*, Yichi Zhang*, <u>Haokun Lin</u>, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li,
      <br>
      <i>in 18th European Conference on Computer Vision (<b>ECCV 2024</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2403.14624">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.14624">arXiv</a>]
      [<a href="https://mathverse-cuhk.github.io/">Project</a>]
      [<a href="https://github.com/ZrrSkywalker/MathVerse">Github</a>]
      [<a href="https://huggingface.co/datasets/AI4Math/MathVerse">Dataset</a>]
      [<a href="https://mp.weixin.qq.com/s/gEcCi92PdMMCItFII84lcw">Synced/æœºå™¨ä¹‹å¿ƒ</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8_jFJjLGaXQJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVW2M:AFWwaeYAAAAAZ1gTQ2Np6tyIjl994arhqzndfoY&scisig=AFWwaeYAAAAAZ1gTQ-MrZ-IrWULY3Og3kaOtlj4&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models.</b> 
      <br>
      Yingtao Zhang, Haoli Bai, <u>Haokun Lin</u>, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci,
      <br>
      <i>in 18th International Conference on Learning Representations (<b>ICLR 2024</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=Tr0lPx9woF">PDF</a>]
      [<a href="https://openreview.net/forum?id=Tr0lPx9woF">OpenReview</a>]
      [<a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:fHzPozkRlIAJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVpi8:AFWwaeYAAAAAZ1gTvi-bdGJkWBWusiXZzPn8hdY&scisig=AFWwaeYAAAAAZ1gTvlueIx_dh4dw2kkRe_TdET0&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
  </tr>


<tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact.</b>
      <br>
      Ruikang Liu, Haoli Bai, <u>Haokun Lin</u>, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan.
      <br>
      <i>In Findings of 62nd Annual Meeting of the Association for Computational Linguistics (<b>ACL 2024 Findings</b>)</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.01241">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.01241">arXiv</a>]
      [<a href="https://github.com/ruikangliu/IntactKV">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:OkR9s_hreeMJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVxTA:AFWwaeYAAAAAZ1gT3TB5Rx3rjLOCG1M3IQaZaXI&scisig=AFWwaeYAAAAAZ1gT3ZncbIhCuf6Goq6jEIx7DwY&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
  </tr>

</table>



# ğŸ† Honors and Awards
- *2024.12*  First Prize, 2024 Graduate Academic Forum, University of Chinese Academy of Sciences.
- *2024.11*  NeurIPS 2024 Top Reviewer Award.
- *2021.06*  Honorary degree of HUST, Top 2%, Highest Honour for Undergraduate.
- *2020.10*  National Scholarship, P.R.China, HUST, Undergraduate Students.
- *2018-2020*  First prize, HUST Excellent Undergraduate Scholarship.
<!-- - *2016.10* Second prize, National (Senior) High School Mathematical Competition of China. -->
   

# ğŸ– Services
- Invited Reviewer:
  - EMNLP'2023, NeurIPS'2024, ICLR'2025, CVPR'2025, AISTATS'2025.
  <!-- - AIM-FM workshop@NeurIPS'2024. -->
  - IEEE Transactions on Neural Networks and Learning Systems (TNNLS).
  - Transactions on Machine Learning Research (TMLR).
- Teaching Assistant:
  - CityU, [CS1315 Computer Programming](https://www.cityu.edu.hk/catalogue/ug/current/course/CS1315.htm), 2024 Fall.
  - CityU, [CS1302 Introduction to Computer Programming](https://www.cityu.edu.hk/catalogue/ug/202021/course/CS1302.htm), 2024 Spring.
  - CityU, [CS5481 Data Engineering](https://www.cityu.edu.hk/catalogue/pg/202425/course/CS5481.htm), 2025 Fall.


# ğŸ’¬ Talks
- Invited talk at the QingKe AI about [DuQuant](https://hcqnc.xetlk.com/sl/2pnEgg).
- Invited talk at the AI Time.

--------

<center><b>Site Analytics</b></center>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=298&t=m&d=DUrSKJKr96ryDYhhGZd-DY-6R_GeZFHFnddY0E2qqII'></script>